{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7b8cc0",
   "metadata": {},
   "source": [
    "- training cho mô hình học liên tục giữa các version, ví dụ: mô hình học 1.3, test trên 1,4 và lại học tiếp trên 1.4, test trên 1.5, học tiếp trên 1.5,....\n",
    "- Dùng mô hình vừa được học trên, test và train tiếp với mô hình khác\n",
    "- Nhớ: scale lại dữ liệu\n",
    "- Trích chọn đặc trưng để tim tham số tốt nhất\n",
    "- Xử lí mất cân bằng dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8534333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file  = ['ant', 'camel', 'ivy','jedit','log4j','lucene','poi','synapse','velocity','xalan','xerces']\n",
    "names = {'ant':['1.3', '1.4', '1.5', '1.6', '1.7']\n",
    "           , 'camel':['1.0', '1.2', '1.4', '1.6']\n",
    "           , 'ivy':['1.0', '1.1', '1.2']\n",
    "           , 'jedit': ['3.2', '4.0', '4.1', '4.2', '4.3']\n",
    "           , 'log4j': ['1.0', '1.1', '1.2']\n",
    "           , 'lucene': ['2.0', '2.2', '2.4']\n",
    "           , 'poi': ['1.5', '2.0', '2.5', '3.0']\n",
    "           , 'synapse': ['1.0', '1.1', '1.2']\n",
    "           ,'velocity': ['1.4', '1.5', '1.6']\n",
    "           ,'xalan': ['2.4' , '2.5', '2.6', '2.7']\n",
    "           ,'xerces': ['1.1', '1.2', '1.3', '1.4.4']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570fdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a004fb5",
   "metadata": {},
   "source": [
    "- KNN : base , smote, selectk, smote & selectk\n",
    "- ensemble: base, smote, selectk, smote & selectk\n",
    "- stacking : base, smote, selectk, smote & selectk\n",
    "- random forest: base, selectk, smote, smote & selectk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45837c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.1786    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.2254    f1_score_train = 0.9873   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.1982    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.5065    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0000    f1_score_train = 1.0000   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.0183    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.3876    f1_score_train = 0.9835   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.3704    f1_score_train = 1.0000   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1944    f1_score_train = 0.9920   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.1739    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1321    f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.6000    f1_score_train = 1.0000   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.5000    f1_score_train = 0.9933   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.4409    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.1778    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.7778   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6462    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.4198    f1_score_train = 1.0000   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6364    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.5556    f1_score_train = 0.9945   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.6212    f1_score_train = 0.9930   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.5812    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.2162    f1_score_train = 0.9643   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.1831    f1_score_train = 0.9863   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.7161    f1_score_train = 0.9960   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2261    f1_score_train = 0.9947   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.1429    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.3876    f1_score_train = 0.9916   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2199    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.7697    f1_score_train = 1.0000   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.5789    f1_score_train = 1.0000   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.2835    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.2140    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.6436    f1_score_train = 0.9974   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.5182    f1_score_train = 0.9951   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#chay model KNN:\n",
    "model = KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1\n",
    ")\n",
    "def binary(y_train):\n",
    "    return (y_train > 0).astype(int)\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        elif file[j] == 'xerces' and names[file[j]][i] == '1.4.4' :\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "        #scale lai du lieu\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5cdd6",
   "metadata": {},
   "source": [
    "xu li mat can bang du lieu bang resampling lai du lieu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af4486",
   "metadata": {},
   "source": [
    "Oversampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e535",
   "metadata": {},
   "source": [
    "Chọn best feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a236381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.3111    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.2130    f1_score_train = 0.9964   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.5091    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5222    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0917    f1_score_train = 1.0000   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.2848    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.3943    f1_score_train = 0.9897   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.4382    f1_score_train = 1.0000   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.5766    f1_score_train = 0.9981   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.1639    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.2000    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.4727    f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.6129    f1_score_train = 1.0000   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5714    f1_score_train = 0.9978   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4211    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.0855    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.1111    f1_score_train = 0.8656   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.7105    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.5154    f1_score_train = 1.0000   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.5948    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.5617    f1_score_train = 0.9952   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6061    f1_score_train = 0.9930   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.3874    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2157    f1_score_train = 0.9643   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.2345    f1_score_train = 0.9982   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.6667    f1_score_train = 0.9960   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2268    f1_score_train = 0.9929   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.4065    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4545    f1_score_train = 0.9969   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.3850    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7301    f1_score_train = 1.0000   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5766    f1_score_train = 1.0000   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.2865    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.3604    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6453    f1_score_train = 0.9976   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.5303    f1_score_train = 0.9958   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6522    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#su dung mo hinh KNN - smote de resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "model = model\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fbcff71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.1967    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.1972    f1_score_train = 0.9873   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.2586    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5183    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.1667    f1_score_train = 1.0000   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.0092    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.4041    f1_score_train = 0.9835   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.3194    f1_score_train = 1.0000   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1667    f1_score_train = 0.9894   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.2044    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0000    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.2456    f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.5767    f1_score_train = 1.0000   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5075    f1_score_train = 0.9933   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4715    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.1923    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.7778   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6866    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4017    f1_score_train = 0.9863   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6338    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.5726    f1_score_train = 0.9775   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6569    f1_score_train = 0.9930   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.5022    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2174    f1_score_train = 0.9603   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.1901    f1_score_train = 0.9863   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7119    f1_score_train = 0.9940   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2353    f1_score_train = 0.9929   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.1739    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4496    f1_score_train = 0.9916   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2065    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7647    f1_score_train = 0.9932   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.6079    f1_score_train = 0.9965   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3293    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.1886    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6651    f1_score_train = 0.9909   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.5279    f1_score_train = 0.9889   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#su dung mo hinh KNN - select k best\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "model = model\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test  = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "           \n",
    "\n",
    "            #chon k best\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fce4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.6454    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.6840    f1_score_train = 0.9982   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.6856    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.7171    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.9194   f1_score_train = 0.9987   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.5055    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.7469    f1_score_train = 0.9643   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.7265    f1_score_train = 1.0000   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.3209   f1_score_train = 0.9959   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.7538    f1_score_train = 0.9909   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.8330    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.6166   f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.6859    f1_score_train = 0.9925   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.6652    f1_score_train = 0.9967   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.7958    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.9447    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.6404   f1_score_train = 0.9910   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6384    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.0735    f1_score_train = 1.0000   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.3058   f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.3558    f1_score_train = 0.9949   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.2949    f1_score_train = 0.9756   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.5078   f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.5492    f1_score_train = 0.9406   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.2047    f1_score_train = 0.9988   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.3080    f1_score_train = 0.9896   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.3467   f1_score_train = 0.9909   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.6110    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.5797    f1_score_train = 0.9955   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2503   f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.2517    f1_score_train = 0.9739   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.4549    f1_score_train = 0.9953   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.7237   f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.3944    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.5246    f1_score_train = 0.9813   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.2396    f1_score_train = 0.9886   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.3063   f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#su dung KNN - chon best feature - smote\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "model = model\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train= scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train_resampled, y_train_resampled = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "            \n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}   f1_score_train = {f1_score(y_train, y_train_pred, average='weighted'):.4f}   \")\n",
    "        \n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            binary(y_test)\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train, y_train = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}    f1_score_train = {f1_score(y_train, y_train_pred, average='weighted'):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f946d67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.0800    f1_score_train = 0.7500   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.1154    f1_score_train = 0.9041   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.1852    f1_score_train = 0.8772   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.5075    f1_score_train = 0.7042   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.1111    f1_score_train = 0.7132   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.0092    f1_score_train = 0.7000   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.3604    f1_score_train = 0.4059   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.2192    f1_score_train = 0.4066   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.0896    f1_score_train = 0.3983   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.2222    f1_score_train = 0.9043   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 0.5455   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1188    f1_score_train = 0.6441   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.5578    f1_score_train = 0.8158   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.4444    f1_score_train = 0.5437   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.4952    f1_score_train = 0.6942   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.2500    f1_score_train = 0.6087   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.5333   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6557    f1_score_train = 0.7636   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.3814    f1_score_train = 0.8065   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6364    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.5357    f1_score_train = 0.8481   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.6430    f1_score_train = 0.9173   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6537    f1_score_train = 0.9115   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.1991    f1_score_train = 0.9023   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.0625    f1_score_train = 0.3556   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.7124    f1_score_train = 0.9427   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2353    f1_score_train = 0.9438   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.0923    f1_score_train = 0.9333   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.4228    f1_score_train = 0.7368   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.1828    f1_score_train = 0.7536   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.7778    f1_score_train = 0.9932   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.5815    f1_score_train = 0.9965   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3188    f1_score_train = 0.5818   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.1095    f1_score_train = 0.5135   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.5598    f1_score_train = 0.7838   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.4491    f1_score_train = 0.7983   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#su dung ensemble leanring\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "models = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        ('gb', GradientBoostingClassifier()),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('svm', SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            models.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = models.predict(X_train_scaled)\n",
    "            y_test_pred = models.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        elif file[j] == 'xerces' and names[file[j]][i] == '1.4.4' :\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "        #scale lai du lieu\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            models.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = models.predict(X_train_scaled)\n",
    "            y_test_pred = models.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0524e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.2373    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.1765    f1_score_train = 0.9873   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.2632    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5116    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.1667    f1_score_train = 1.0000   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.0183    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.3655    f1_score_train = 0.9764   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.3481    f1_score_train = 0.9965   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1667    f1_score_train = 0.9894   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.2119    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0000    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1495    f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.6036    f1_score_train = 1.0000   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.4962    f1_score_train = 0.9933   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4516    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.1818    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.7778   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6462    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4262    f1_score_train = 0.9863   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6316    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.6367    f1_score_train = 0.9945   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6182    f1_score_train = 0.9930   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6798    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2271    f1_score_train = 0.9600   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.1831    f1_score_train = 0.9863   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7130    f1_score_train = 0.9940   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2443    f1_score_train = 0.9929   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.1714    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4094    f1_score_train = 0.9916   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.3582    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7742    f1_score_train = 0.9966   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.6018    f1_score_train = 0.9965   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3574    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.1858    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6698    f1_score_train = 0.9909   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.5315    f1_score_train = 0.9939   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#ensemble - selectkbest\n",
    "models_select = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        ('gb', GradientBoostingClassifier()),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('svm', SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test  = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "           \n",
    "\n",
    "            #chon k best\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b14c935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.2785    f1_score_train = 0.9904   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.3063    f1_score_train = 0.9853   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.5098    f1_score_train = 0.9476   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5634    f1_score_train = 0.9189   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0755    f1_score_train = 0.8716   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.1712    f1_score_train = 0.9860   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.4343    f1_score_train = 0.7598   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.3826    f1_score_train = 0.8167   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.3810    f1_score_train = 0.8919   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.2330    f1_score_train = 0.8545   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.1739    f1_score_train = 0.9842   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.4741    f1_score_train = 0.9320   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.6279    f1_score_train = 0.9042   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.6144    f1_score_train = 0.9309   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4965    f1_score_train = 0.9116   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.1111    f1_score_train = 0.9186   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.9876   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6957    f1_score_train = 0.8962   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4408    f1_score_train = 0.9091   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6212    f1_score_train = 0.9075   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.5614    f1_score_train = 0.8817   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6287    f1_score_train = 0.9008   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6121    f1_score_train = 0.8400   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2051    f1_score_train = 0.8760   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.1944    f1_score_train = 0.9367   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.6903    f1_score_train = 0.9217   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.3380    f1_score_train = 0.9006   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.3778    f1_score_train = 0.9819   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4234    f1_score_train = 0.8949   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.3107    f1_score_train = 0.9032   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7605    f1_score_train = 0.9932   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5730    f1_score_train = 0.8216   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3458    f1_score_train = 0.8978   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.3320    f1_score_train = 0.9045   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6103    f1_score_train = 0.7983   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.4895    f1_score_train = 0.8354   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6468    f1_score_train = 0.9933   \n"
     ]
    }
   ],
   "source": [
    "#ensemble - smote\n",
    "models_over = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        ('gb', GradientBoostingClassifier()),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('svm', SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "            models_over.fit(X_train, y_train)\n",
    "            y_train_pred = models_over.predict(X_train)\n",
    "            y_test_pred = models_over.predict(X_test)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            models_over.fit(X_train, y_train)\n",
    "            y_train_pred = models_over.predict(X_train)\n",
    "            y_test_pred = models_over.predict(X_test)\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a46ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.6641    f1_score_train = 0.9976   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.7759    f1_score_train = 0.9964   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.6722    f1_score_train = 0.9343   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.7127    f1_score_train = 0.8936   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.9354   f1_score_train = 0.8773   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.5060    f1_score_train = 0.9752   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.7518    f1_score_train = 0.8741   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.7170    f1_score_train = 0.9198   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.2805   f1_score_train = 0.7923   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.8274    f1_score_train = 0.9354   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.8330    f1_score_train = 0.9550   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.5740   f1_score_train = 0.9307   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.6855    f1_score_train = 0.9503   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.6494    f1_score_train = 0.9831   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.8169    f1_score_train = 0.9593   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.9584    f1_score_train = 0.9857   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.6404   f1_score_train = 0.9792   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6161    f1_score_train = 0.9199   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.0528    f1_score_train = 0.9007   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.2970   f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.3173    f1_score_train = 0.7961   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.3080    f1_score_train = 0.8645   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6371   f1_score_train = 0.9009   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.6216    f1_score_train = 0.7366   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.2189    f1_score_train = 0.9157   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.2804    f1_score_train = 0.8592   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.4818   f1_score_train = 0.9462   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.6151    f1_score_train = 0.9564   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.5806    f1_score_train = 0.9125   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.1793   f1_score_train = 0.8283   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.2268    f1_score_train = 0.8552   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.4876    f1_score_train = 0.8447   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.8023   f1_score_train = 0.7643   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.3585    f1_score_train = 0.8727   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.5464    f1_score_train = 0.6279   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.1102    f1_score_train = 0.7148   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.3063   f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "models_combine = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "        ('gb', GradientBoostingClassifier()),\n",
    "        ('nb', GaussianNB()),\n",
    "        ('svm', SVC(probability=True))\n",
    "    ]\n",
    ")\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train= scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train_resampled, y_train_resampled = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "            \n",
    "\n",
    "            models_combine.fit(X_train, y_train)\n",
    "            y_train_pred = models_combine.predict(X_train)\n",
    "            y_test_pred = models_combine.predict(X_test)\n",
    "\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}   f1_score_train = {f1_score(y_train, y_train_pred, average='weighted'):.4f}   \")\n",
    "        \n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            binary(y_test)\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train, y_train = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            models_combine.fit(X_train, y_train)\n",
    "            y_train_pred = models_combine.predict(X_train)\n",
    "            y_test_pred = models_combine.predict(X_test)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}    f1_score_train = {f1_score(y_train, y_train_pred, average='weighted'):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5599a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.0870    f1_score_train = 0.6207   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.0455    f1_score_train = 0.9474   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.2385    f1_score_train = 0.5778   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5502    f1_score_train = 0.9721   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0000    f1_score_train = 0.8822   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.3502    f1_score_train = 0.7657   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.2110    f1_score_train = 0.4409   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1194    f1_score_train = 0.4715   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.1655    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1569    f1_score_train = 0.6441   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.5732    f1_score_train = 0.9944   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5039    f1_score_train = 0.9726   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.5091    f1_score_train = 0.7442   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.2632    f1_score_train = 0.8293   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6875    f1_score_train = 0.7018   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4659    f1_score_train = 0.8060   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6408    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.5628    f1_score_train = 0.8072   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6841    f1_score_train = 0.9662   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.7220    f1_score_train = 0.9975   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2118    f1_score_train = 0.9030   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.1527    f1_score_train = 0.8788   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7319    f1_score_train = 0.9940   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2174    f1_score_train = 0.9947   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.0000    f1_score_train = 0.6087   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4355    f1_score_train = 0.9091   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2513    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7683    f1_score_train = 0.9574   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5887    f1_score_train = 1.0000   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3230    f1_score_train = 0.9935   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.1268    f1_score_train = 0.7429   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6598    f1_score_train = 0.9897   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.6363    f1_score_train = 0.9841   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, train_test_split\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "stacking_clf = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test  = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "            stacking_clf.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = stacking_clf.predict(X_train_scaled)\n",
    "            y_test_pred = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "           \n",
    "\n",
    "            #chon k best\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            stacking_clf.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = stacking_clf.predict(X_train_scaled)\n",
    "            y_test_pred = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c8831",
   "metadata": {},
   "source": [
    "thử thêm với PCA thay cho select k bét, xem keqt qua nnao\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df8fffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.1176    f1_score_train = 0.6207   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.1224    f1_score_train = 0.9610   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.2545    f1_score_train = 0.6383   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5311    f1_score_train = 0.8712   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.1818    f1_score_train = 0.8630   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.4538    f1_score_train = 0.9811   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.3130    f1_score_train = 0.8760   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1714    f1_score_train = 0.9379   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.1926    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.0619    f1_score_train = 0.6441   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.5509    f1_score_train = 0.9888   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5116    f1_score_train = 0.9655   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.5357    f1_score_train = 0.8345   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.2632    f1_score_train = 0.8293   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6875    f1_score_train = 0.6786   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4390    f1_score_train = 0.8529   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6386    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.5887    f1_score_train = 0.7407   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6667    f1_score_train = 0.9930   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.7378    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2188    f1_score_train = 0.9333   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.1837    f1_score_train = 0.9863   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7253    f1_score_train = 0.9940   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2158    f1_score_train = 0.9911   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.3968    f1_score_train = 0.9831   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.1871    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7647    f1_score_train = 0.9542   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5887    f1_score_train = 0.9965   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3173    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.1050    f1_score_train = 0.6585   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6807    f1_score_train = 0.9869   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.6012    f1_score_train = 0.9781   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#stacking - selectkbest\n",
    "base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "stacking_select = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test  = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "            stacking_clf.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = stacking_clf.predict(X_train_scaled)\n",
    "            y_test_pred = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "           \n",
    "\n",
    "            #chon k best\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            stacking_clf.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = stacking_clf.predict(X_train_scaled)\n",
    "            y_test_pred = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9661ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.2121    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.2326    f1_score_train = 0.9964   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.3308    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5282    f1_score_train = 1.0000   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0625    f1_score_train = 1.0000   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.1317    f1_score_train = 1.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.4703    f1_score_train = 0.9897   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.4545    f1_score_train = 1.0000   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.3457    f1_score_train = 0.9981   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.2031    f1_score_train = 0.9920   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0800    f1_score_train = 1.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.2931    f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.6180    f1_score_train = 1.0000   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5811    f1_score_train = 0.9978   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4748    f1_score_train = 1.0000   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.1538    f1_score_train = 1.0000   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.9918   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6857    f1_score_train = 1.0000   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4578    f1_score_train = 1.0000   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6383    f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.6316    f1_score_train = 0.9810   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6565    f1_score_train = 0.9931   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.5862    f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2379    f1_score_train = 0.9645   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.2138    f1_score_train = 0.9982   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7140    f1_score_train = 0.9960   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2295    f1_score_train = 0.9947   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.2597    f1_score_train = 1.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4030    f1_score_train = 0.9969   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.3000    f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7742    f1_score_train = 1.0000   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5887    f1_score_train = 1.0000   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3221    f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.2804    f1_score_train = 1.0000   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6744    f1_score_train = 0.9976   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.6269    f1_score_train = 0.9958   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6387    f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#stacking - smote\n",
    "base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "stacking_over = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "            stacking_over.fit(X_train, y_train)\n",
    "            y_train_pred = stacking_over.predict(X_train)\n",
    "            y_test_pred = stacking_over.predict(X_test)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "            stacking_over.fit(X_train, y_train)\n",
    "            y_train_pred = stacking_over.predict(X_train)\n",
    "            y_test_pred = stacking_over.predict(X_test)\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2c841e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.6919    f1_score_train = 1.0000   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.8038    f1_score_train = 0.9982   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.6540    f1_score_train = 1.0000   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.7060    f1_score_train = 0.8453   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.9375   f1_score_train = 0.9991   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.5094    f1_score_train = 0.9399   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.7614    f1_score_train = 0.6871   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.7203    f1_score_train = 0.8096   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.4594   f1_score_train = 0.9974   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.7773    f1_score_train = 0.6450   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.8330    f1_score_train = 0.9016   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.7106   f1_score_train = 1.0000   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.6553    f1_score_train = 0.7133   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.6548    f1_score_train = 0.8773   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.8203    f1_score_train = 0.7457   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.9638    f1_score_train = 0.8889   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.6367   f1_score_train = 0.9917   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.5457    f1_score_train = 0.6911   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.0804    f1_score_train = 0.6257   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.3355   f1_score_train = 1.0000   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.2987    f1_score_train = 0.5080   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.2751    f1_score_train = 0.5063   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6070   f1_score_train = 1.0000   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.4924    f1_score_train = 0.7422   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.2223    f1_score_train = 0.9988   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.3118    f1_score_train = 0.8541   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.4222   f1_score_train = 0.9947   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.6151    f1_score_train = 0.8524   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.5687    f1_score_train = 0.9593   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.3156   f1_score_train = 1.0000   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.2114    f1_score_train = 0.9200   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.4516    f1_score_train = 0.7666   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.7315   f1_score_train = 1.0000   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.3756    f1_score_train = 0.8858   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.5331    f1_score_train = 0.9002   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.2703    f1_score_train = 0.9209   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.3036   f1_score_train = 1.0000   \n"
     ]
    }
   ],
   "source": [
    "#stacking - smote - select\n",
    "base_models = [\n",
    "            ('logreg', LogisticRegression(class_weight='balanced', random_state=42)),\n",
    "            ('svc', SVC(class_weight='balanced', probability=True, random_state=42)),\n",
    "            ('rf', RandomForestClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),\n",
    "            ('knn', KNeighborsClassifier(n_neighbors=7,\n",
    "    weights='distance',\n",
    "    metric='minkowski',\n",
    "    p=2,\n",
    "    algorithm='auto',\n",
    "    n_jobs=-1)),\n",
    "            ('ada', AdaBoostClassifier(random_state=42)),\n",
    "            ('nb', GaussianNB())\n",
    "        ]\n",
    "\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "        # Sử dụng StackingClassifier\n",
    "stacking_combine = StackingClassifier(\n",
    "            estimators=base_models, \n",
    "            final_estimator=meta_model,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train= scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train_resampled, y_train_resampled = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "            stacking_combine.fit(X_train_resampled, y_train_resampled)\n",
    "            y_train_pred = stacking_combine.predict(X_train_resampled)\n",
    "            y_test_pred = stacking_combine.predict(X_test)\n",
    "\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}   f1_score_train = {f1_score(y_train_resampled, y_train_pred, average='weighted'):.4f}   \")\n",
    "        \n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            binary(y_test)\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train_resampled, y_train_resampled = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            stacking_combine.fit(X_train_resampled, y_train_resampled)\n",
    "            y_train_pred = stacking_combine.predict(X_train_resampled)\n",
    "            y_test_pred = stacking_combine.predict(X_test)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred, average='weighted'):.4f}    f1_score_train = {f1_score(y_train_resampled, y_train_pred, average='weighted'):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c46128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.0851    f1_score_train = 0.3333   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.0606    f1_score_train = 0.0488   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.1714    f1_score_train = 0.4762   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.5570    f1_score_train = 0.7018   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0870    f1_score_train = 0.6877   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.3932    f1_score_train = 0.6000   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.1273    f1_score_train = 0.2989   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1739    f1_score_train = 0.3504   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.1854    f1_score_train = 0.8120   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1553    f1_score_train = 0.3600   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.5595    f1_score_train = 0.7574   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.5920    f1_score_train = 0.6116   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.5124    f1_score_train = 0.6815   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.2353    f1_score_train = 0.4308   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6875    f1_score_train = 0.6786   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.4083    f1_score_train = 0.7812   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6364    f1_score_train = 0.9594   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.6050    f1_score_train = 0.7619   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.7261    f1_score_train = 0.8258   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.7595    f1_score_train = 0.8571   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.2105    f1_score_train = 0.8690   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.0472    f1_score_train = 0.2381   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.7766    f1_score_train = 0.9026   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2137    f1_score_train = 0.8897   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.4032    f1_score_train = 0.6465   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2637    f1_score_train = 0.7152   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.7978    f1_score_train = 0.9091   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.5506    f1_score_train = 0.8439   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3975    f1_score_train = 0.6861   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.1229    f1_score_train = 0.4444   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.6580    f1_score_train = 0.7967   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.5858    f1_score_train = 0.8344   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 0.9939   \n"
     ]
    }
   ],
   "source": [
    "#chay model Rf\n",
    "model = RandomForestClassifier(n_estimators = 30,\n",
    "    max_depth = 27,\n",
    "    min_samples_split = 7,\n",
    "    min_samples_leaf =10,\n",
    "    max_features ='log2'\n",
    "    )\n",
    "def binary(y_train):\n",
    "    return (y_train > 0).astype(int)\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        elif file[j] == 'xerces' and names[file[j]][i] == '1.4.4' :\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "        #scale lai du lieu\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a862a9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.3373    f1_score_train = 0.9057   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.2982    f1_score_train = 0.8705   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.5359    f1_score_train = 0.9533   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.6103    f1_score_train = 0.8577   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.1277    f1_score_train = 0.8917   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.2137    f1_score_train = 0.9817   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.4365    f1_score_train = 0.8508   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.4197    f1_score_train = 0.9215   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.4421    f1_score_train = 0.8985   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.1884    f1_score_train = 0.7969   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.3810    f1_score_train = 0.9453   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.6092    f1_score_train = 0.9139   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.5864    f1_score_train = 0.8723   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5838    f1_score_train = 0.8522   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4848    f1_score_train = 0.8908   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.1333    f1_score_train = 0.9181   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0571    f1_score_train = 0.9726   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.7105    f1_score_train = 0.8824   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.5564    f1_score_train = 0.8707   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6406    f1_score_train = 0.9657   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.6173    f1_score_train = 0.7959   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.6650    f1_score_train = 0.8369   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.6473    f1_score_train = 0.8507   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2172    f1_score_train = 0.8247   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.3115    f1_score_train = 0.9389   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7471    f1_score_train = 0.8815   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2240    f1_score_train = 0.8697   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.4615    f1_score_train = 0.8942   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.5443    f1_score_train = 0.8571   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.4935    f1_score_train = 0.8580   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7836    f1_score_train = 0.9161   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5614    f1_score_train = 0.8271   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.4077    f1_score_train = 0.8581   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.4340    f1_score_train = 0.9101   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6890    f1_score_train = 0.8044   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.6269    f1_score_train = 0.8697   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6387    f1_score_train = 0.9983   \n"
     ]
    }
   ],
   "source": [
    "#su dung mo hinh rf - smote de resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 30,\n",
    "    max_depth = 27,\n",
    "    min_samples_split = 7,\n",
    "    min_samples_leaf =10,\n",
    "    max_features ='log2'\n",
    "    )\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test  =scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba06478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on camel-1.4: f1_score = 0.1852    f1_score_train = 0.4000   \n",
      "Training on ant-1.4 & testing on camel-1.5: f1_score = 0.1176    f1_score_train = 0.0000   \n",
      "Training on ant-1.5 & testing on camel-1.6: f1_score = 0.3500    f1_score_train = 0.4681   \n",
      "Training on ant-1.6 & testing on camel-1.7: f1_score = 0.5833    f1_score_train = 0.7024   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0909    f1_score_train = 0.6551   \n",
      "Training on camel-1.0 & testing on ivy-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on camel-1.2 & testing on ivy-1.4: f1_score = 0.3701    f1_score_train = 0.5663   \n",
      "Training on camel-1.4 & testing on ivy-1.6: f1_score = 0.1843    f1_score_train = 0.3464   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1972    f1_score_train = 0.2920   \n",
      "Training on ivy-1.0 & testing on jedit-1.1: f1_score = 0.1830    f1_score_train = 0.8000   \n",
      "Training on ivy-1.1 & testing on jedit-1.2: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1553    f1_score_train = 0.3846   \n",
      "Training on jedit-3.2 & testing on log4j-4.0: f1_score = 0.5357    f1_score_train = 0.7470   \n",
      "Training on jedit-4.0 & testing on log4j-4.1: f1_score = 0.5354    f1_score_train = 0.6400   \n",
      "Training on jedit-4.1 & testing on log4j-4.2: f1_score = 0.4793    f1_score_train = 0.6906   \n",
      "Training on jedit-4.2 & testing on log4j-4.3: f1_score = 0.2857    f1_score_train = 0.4478   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on log4j-1.0 & testing on lucene-1.1: f1_score = 0.6562    f1_score_train = 0.7018   \n",
      "Training on log4j-1.1 & testing on lucene-1.2: f1_score = 0.4344    f1_score_train = 0.7385   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6364    f1_score_train = 0.9594   \n",
      "Training on lucene-2.0 & testing on poi-2.2: f1_score = 0.6016    f1_score_train = 0.7614   \n",
      "Training on lucene-2.2 & testing on poi-2.4: f1_score = 0.7392    f1_score_train = 0.8333   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.7192    f1_score_train = 0.8558   \n",
      "Training on poi-1.5 & testing on synapse-2.0: f1_score = 0.2176    f1_score_train = 0.8429   \n",
      "Training on poi-2.0 & testing on synapse-2.5: f1_score = 0.0692    f1_score_train = 0.3333   \n",
      "Training on poi-2.5 & testing on synapse-3.0: f1_score = 0.7735    f1_score_train = 0.9041   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2222    f1_score_train = 0.8857   \n",
      "Training on synapse-1.0 & testing on velocity-1.1: f1_score = 0.0000    f1_score_train = 0.0000   \n",
      "Training on synapse-1.1 & testing on velocity-1.2: f1_score = 0.4341    f1_score_train = 0.6535   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2682    f1_score_train = 0.6351   \n",
      "Training on velocity-1.4 & testing on xalan-1.5: f1_score = 0.7966    f1_score_train = 0.9119   \n",
      "Training on velocity-1.5 & testing on xalan-1.6: f1_score = 0.5425    f1_score_train = 0.8750   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3736    f1_score_train = 0.6571   \n",
      "Training on xalan-2.4 & testing on xerces-2.5: f1_score = 0.1190    f1_score_train = 0.4247   \n",
      "Training on xalan-2.5 & testing on xerces-2.6: f1_score = 0.6933    f1_score_train = 0.7873   \n",
      "Training on xalan-2.6 & testing on xerces-2.7: f1_score = 0.6056    f1_score_train = 0.8088   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444    f1_score_train = 0.9939   \n"
     ]
    }
   ],
   "source": [
    "#su dung mo hinh rf - select k best\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "model = RandomForestClassifier(n_estimators = 30,\n",
    "    max_depth = 27,\n",
    "    min_samples_split = 7,\n",
    "    min_samples_leaf =10,\n",
    "    max_features ='log2'\n",
    "    )\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test  = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            \n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "           \n",
    "\n",
    "            #chon k best\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled  =scaler.transform(X_test)\n",
    "\n",
    "\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            y_train_pred = model.predict(X_train_scaled)\n",
    "            y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8695a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on ant-1.3 & testing on ant-1.4: f1_score = 0.3678    f1_score_train = 0.8879   \n",
      "Training on ant-1.4 & testing on ant-1.5: f1_score = 0.3670    f1_score_train = 0.8252   \n",
      "Training on ant-1.5 & testing on ant-1.6: f1_score = 0.4875    f1_score_train = 0.9140   \n",
      "Training on ant-1.6 & testing on ant-1.7: f1_score = 0.5939    f1_score_train = 0.8588   \n",
      "Training on ant-1.7 & testing on camel-1.0: f1_score = 0.0952   f1_score_train = 0.6643   \n",
      "Training on camel-1.0 & testing on camel-1.2: f1_score = 0.2838    f1_score_train = 0.9428   \n",
      "Training on camel-1.2 & testing on camel-1.4: f1_score = 0.4311    f1_score_train = 0.8415   \n",
      "Training on camel-1.4 & testing on camel-1.6: f1_score = 0.3676    f1_score_train = 0.8964   \n",
      "Training on camel-1.6 & testing on ivy-1.0: f1_score = 0.1714   f1_score_train = 0.3043   \n",
      "Training on ivy-1.0 & testing on ivy-1.1: f1_score = 0.1860    f1_score_train = 0.7869   \n",
      "Training on ivy-1.1 & testing on ivy-1.2: f1_score = 0.4340    f1_score_train = 0.9247   \n",
      "Training on ivy-1.2 & testing on jedit-3.2: f1_score = 0.1569   f1_score_train = 0.3846   \n",
      "Training on jedit-3.2 & testing on jedit-4.0: f1_score = 0.5596    f1_score_train = 0.8548   \n",
      "Training on jedit-4.0 & testing on jedit-4.1: f1_score = 0.6092    f1_score_train = 0.8652   \n",
      "Training on jedit-4.1 & testing on jedit-4.2: f1_score = 0.4734    f1_score_train = 0.8363   \n",
      "Training on jedit-4.2 & testing on jedit-4.3: f1_score = 0.1200    f1_score_train = 0.9150   \n",
      "Training on jedit-4.3 & testing on log4j-1.0: f1_score = 0.0000   f1_score_train = 0.0000   \n",
      "Training on log4j-1.0 & testing on log4j-1.1: f1_score = 0.6400    f1_score_train = 0.8571   \n",
      "Training on log4j-1.1 & testing on log4j-1.2: f1_score = 0.5000    f1_score_train = 0.8345   \n",
      "Training on log4j-1.2 & testing on lucene-2.0: f1_score = 0.6364   f1_score_train = 0.9594   \n",
      "Training on lucene-2.0 & testing on lucene-2.2: f1_score = 0.5774    f1_score_train = 0.7739   \n",
      "Training on lucene-2.2 & testing on lucene-2.4: f1_score = 0.6495    f1_score_train = 0.8162   \n",
      "Training on lucene-2.4 & testing on poi-1.5: f1_score = 0.7453   f1_score_train = 0.8625   \n",
      "Training on poi-1.5 & testing on poi-2.0: f1_score = 0.2254    f1_score_train = 0.8315   \n",
      "Training on poi-2.0 & testing on poi-2.5: f1_score = 0.3384    f1_score_train = 0.9142   \n",
      "Training on poi-2.5 & testing on poi-3.0: f1_score = 0.7329    f1_score_train = 0.8954   \n",
      "Training on poi-3.0 & testing on synapse-1.0: f1_score = 0.2344   f1_score_train = 0.8953   \n",
      "Training on synapse-1.0 & testing on synapse-1.1: f1_score = 0.4528    f1_score_train = 0.8836   \n",
      "Training on synapse-1.1 & testing on synapse-1.2: f1_score = 0.4286    f1_score_train = 0.8834   \n",
      "Training on synapse-1.2 & testing on velocity-1.4: f1_score = 0.2796   f1_score_train = 0.6951   \n",
      "Training on velocity-1.4 & testing on velocity-1.5: f1_score = 0.7619    f1_score_train = 0.9139   \n",
      "Training on velocity-1.5 & testing on velocity-1.6: f1_score = 0.5752    f1_score_train = 0.8374   \n",
      "Training on velocity-1.6 & testing on xalan-2.4: f1_score = 0.3713   f1_score_train = 0.6986   \n",
      "Training on xalan-2.4 & testing on xalan-2.5: f1_score = 0.4406    f1_score_train = 0.9068   \n",
      "Training on xalan-2.5 & testing on xalan-2.6: f1_score = 0.6862    f1_score_train = 0.8201   \n",
      "Training on xalan-2.6 & testing on xalan-2.7: f1_score = 0.6280    f1_score_train = 0.8484   \n",
      "Training on xalan-2.7 & testing on xerces-1.1: f1_score = 0.6444   f1_score_train = 0.9939   \n"
     ]
    }
   ],
   "source": [
    "#su dung rf - chon best feature - smote\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "model = RandomForestClassifier(n_estimators = 30,\n",
    "    max_depth = 27,\n",
    "    min_samples_split = 7,\n",
    "    min_samples_leaf =10,\n",
    "    max_features ='log2'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(file)-1):\n",
    "    for i in range(len(names[file[j]])):\n",
    "        if i  == len(names[file[j]])-1:\n",
    "            data_train  = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j+1]}/{file[j+1]}-{names[file[j+1]][0]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train= scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train_resampled, y_train_resampled = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "            \n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j+1]}-{names[file[j+1]][0]}: f1_score = {f1_score(y_test, y_test_pred):.4f}   f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "        \n",
    "\n",
    "        else:\n",
    "            data_train = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i]}.csv\")\n",
    "            X_train = data_train.drop(['name','bug'], axis = 1)\n",
    "            y_train = data_train['bug']\n",
    "\n",
    "            y_train = binary(y_train)\n",
    "\n",
    "            data_test = pd.read_csv(f\"C:/Users/ACER/Downloads/{file[j]}/{file[j]}-{names[file[j]][i + 1]}.csv\")\n",
    "            X_test = data_test.drop(['name','bug'], axis =1)\n",
    "            y_test = data_test['bug']\n",
    "\n",
    "            y_test = binary(y_test)\n",
    "\n",
    "            #chon k feature tot nhat\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "            X_train = selector.fit_transform(X_train, y_train)\n",
    "            X_test = selector.transform(X_test)\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            #oversampling lai du lieu tren tap train\n",
    "            oversample = SMOTE()\n",
    "            minority_class = min(Counter(y_train), key=Counter(y_train).get)\n",
    "            minority_count = Counter(y_train)[minority_class]\n",
    "            if minority_count <= 1:\n",
    "                X_train, y_train = X_train, y_train\n",
    "            else:\n",
    "                k = min(5, minority_count - 1)\n",
    "                oversample = SMOTE(k_neighbors=k)\n",
    "                X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "            y_train_pred = model.predict(X_train)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "            print(f\"Training on {file[j]}-{names[file[j]][i]} & testing on {file[j]}-{names[file[j]][i+1]}: f1_score = {f1_score(y_test, y_test_pred):.4f}    f1_score_train = {f1_score(y_train, y_train_pred):.4f}   \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680fba54",
   "metadata": {},
   "source": [
    "Chạy mô hình đối với các bộ project của data NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b6e79",
   "metadata": {},
   "source": [
    "GFS to select a subset of features that maximize classification performance.\n",
    "Implementation:\n",
    "- For each training dataset (e.g., KC3), start with an empty feature set.\n",
    "- Iteratively add the feature that most improves the F1-score (or AUC) on a validation set within the training data.\n",
    "- Stop when adding more features does not improve performance significantly (e.g., based on a threshold or fixed number of features).\n",
    "- Apply the selected features to both training (e.g., KC3) and testing (e.g., MC1) datasets, ensuring only common features are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836e2055",
   "metadata": {},
   "source": [
    "Đổi sang model ensemble leanring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
