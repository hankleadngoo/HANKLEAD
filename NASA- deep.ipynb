{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bdba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CM1: F1-score trung bình = 0.3801\n",
      "KC1: F1-score trung bình = 0.4788\n",
      "KC3: F1-score trung bình = 0.3876\n",
      "MC1: F1-score trung bình = 0.0830\n",
      "WARNING:tensorflow:5 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002EB24335620> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002EB267E6E80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "MC2: F1-score trung bình = 0.6215\n",
      "MW1: F1-score trung bình = 0.3789\n",
      "PC1: F1-score trung bình = 0.2889\n",
      "PC3: F1-score trung bình = 0.4377\n",
      "PC4: F1-score trung bình = 0.5791\n",
      "PC5: F1-score trung bình = 0.5162\n",
      "\n",
      "=== Tổng hợp ===\n",
      "CM1: 0.3801\n",
      "KC1: 0.4788\n",
      "KC3: 0.3876\n",
      "MC1: 0.0830\n",
      "MC2: 0.6215\n",
      "MW1: 0.3789\n",
      "PC1: 0.2889\n",
      "PC3: 0.4377\n",
      "PC4: 0.5791\n",
      "PC5: 0.5162\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class DNN:\n",
    "    def __init__(self, input_dim, hidden_layers,\n",
    "                 activation=\"relu\", output_activation=\"sigmoid\",\n",
    "                 learning_rate=0.001, alpha=0.5, gamma=2.0, loss=None,\n",
    "                 optimizer=\"adam\"):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss if loss is not None else self.focal_loss\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def focal_loss(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        cross_entropy = -(y_true * tf.math.log(y_pred) +\n",
    "                          (1 - y_true) * tf.math.log(1 - y_pred))\n",
    "        prob_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        focal_weight = self.alpha * tf.pow(1 - prob_t, self.gamma)\n",
    "        return tf.reduce_mean(focal_weight * cross_entropy)\n",
    "\n",
    "    def _get_optimizer(self):\n",
    "        if isinstance(self.optimizer, str):\n",
    "            opt = self.optimizer.lower()\n",
    "            if opt == \"adam\":\n",
    "                return keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "            elif opt == \"sgd\":\n",
    "                return keras.optimizers.SGD(learning_rate=self.learning_rate, momentum=0.9)\n",
    "            elif opt == \"rmsprop\":\n",
    "                return keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n",
    "            elif opt == \"adagrad\":\n",
    "                return keras.optimizers.Adagrad(learning_rate=self.learning_rate)\n",
    "            else:\n",
    "                raise ValueError(f\"Optimizer '{self.optimizer}' không được hỗ trợ!\")\n",
    "        else:\n",
    "            return self.optimizer\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.Input(shape=(self.input_dim,)))\n",
    "        for units in self.hidden_layers:\n",
    "            model.add(layers.Dense(units, activation=self.activation))\n",
    "        model.add(layers.Dense(1, activation=self.output_activation))\n",
    "        model.compile(optimizer=self._get_optimizer(),\n",
    "                      loss=self.loss,\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=0):\n",
    "        return self.model.fit(X_train, y_train,\n",
    "                              epochs=epochs,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_split=validation_split,\n",
    "                              verbose=verbose,\n",
    "                              shuffle=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X, verbose=0)\n",
    "\n",
    "\n",
    "class DNNStacking:\n",
    "    def __init__(self, model1_params, model2_params):\n",
    "        self.model1 = DNN(**model1_params)\n",
    "        self.model2 = DNN(**model2_params)\n",
    "        self.meta = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs=30, batch_size=32, verbose=0):\n",
    "        self.model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        self.model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "        pred1 = self.model1.predict(X_train).reshape(-1, 1)\n",
    "        pred2 = self.model2.predict(X_train).reshape(-1, 1)\n",
    "        X_meta = np.hstack([pred1, pred2])\n",
    "        self.meta.fit(X_meta, y_train)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred1 = self.model1.predict(X).reshape(-1, 1)\n",
    "        pred2 = self.model2.predict(X).reshape(-1, 1)\n",
    "        X_meta = np.hstack([pred1, pred2])\n",
    "        return self.meta.predict(X_meta)\n",
    "\n",
    "NASAfile = ['CM1', 'KC1', 'KC3', 'MC1', 'MC2', 'MW1',\n",
    "            'PC1', 'PC3', 'PC4', 'PC5']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for file in NASAfile:\n",
    "    data = pd.read_csv(f\"C:/NASA/{file}.csv\")\n",
    "    data['Defective'] = data['Defective'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "    X = data.drop(['Defective'], axis=1).values\n",
    "    y = data['Defective'].values\n",
    "\n",
    "    # Chuẩn hóa\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # chọn 10 đặc trưng tốt nhất\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "        X_train = selector.fit_transform(X_train, y_train)\n",
    "        X_test = selector.transform(X_test)\n",
    "\n",
    "        # SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # tham số DNN\n",
    "        params1 = {\"input_dim\": X_train.shape[1], \"hidden_layers\": [128, 64], \"learning_rate\": 0.001}\n",
    "        params2 = {\"input_dim\": X_train.shape[1], \"hidden_layers\": [256, 128], \"learning_rate\": 0.001}\n",
    "\n",
    "        # Train stacking\n",
    "        stack_model = DNNStacking(params1, params2)\n",
    "        stack_model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "        y_pred = stack_model.predict(X_test)\n",
    "        f1_scores.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    results[file] = avg_f1\n",
    "    print(f\"{file}: F1-score trung bình = {avg_f1:.4f}\")\n",
    "\n",
    "print(\"\\n=== Tổng hợp ===\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70088f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
